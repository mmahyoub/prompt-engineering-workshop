{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdq1eOaZ6S81s4XCuEFXnZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmahyoub/prompt-engineering-workshop/blob/main/Advanced_Prompt_Engineering_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced Prompt Engineering Techniques\n"
      ],
      "metadata": {
        "id": "MzB_qNbn5_-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "!pip install openai langchain langchain_openai langchain_community langchain-chroma -q"
      ],
      "metadata": {
        "id": "fLxWGAN36J1x"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "api_key = getpass(\"Enter your OpenAI API Key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU7Pp2cq7OIt",
        "outputId": "0a514c0b-c0bd-4a35-9c53-f50104231e44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "vEH34OFL-YKP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ],
      "metadata": {
        "id": "FZAy690u8UcF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "AQnQE_2U8snN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "def display_text(text):\n",
        "  display(Markdown(text))"
      ],
      "metadata": {
        "id": "S7IC4isqOPLj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### One-Shot Prompting"
      ],
      "metadata": {
        "id": "X_5Mvb2p6Ed3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> One-shot prompting involves providing a single example to guide the model's response:"
      ],
      "metadata": {
        "id": "07F2vSff88kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"\"\"Generate a creative name for a {product} company. For example, a coffee company could be named 'Bean Voyage'.\n",
        "\n",
        "    Company name: \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "n8tO0M7N6EPu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Prompt:\")\n",
        "display_text(prompt.invoke({\"product\": \"coffee\"}).text)\n",
        "\n",
        "# LLM chain\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "print(\"\\nOutput:\")\n",
        "display_text(chain.invoke({\"product\": \"coffee\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "u4LFfWtq_Kab",
        "outputId": "6aa13053-06db-4302-a283-455781feea55"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Generate a creative name for a coffee company. For example, a coffee company could be named 'Bean Voyage'.\n    \n    Company name: "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Brewed Awakening"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Few-Shot Prompting"
      ],
      "metadata": {
        "id": "xCFaVHOnPaTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few-shot prompting uses multiple examples to guide the model."
      ],
      "metadata": {
        "id": "-peR7foISXtX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JherMBvY5sH7"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\"animal\": \"cat\", \"description\": \"A furry feline with retractable claws and independent nature.\"},\n",
        "    {\"animal\": \"elephant\", \"description\": \"A large mammal with a long trunk and tusks, known for its intelligence.\"},\n",
        "    {\"animal\": \"penguin\", \"description\": \"A flightless bird adapted to swimming, with a tuxedo-like appearance.\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = PromptTemplate.from_template(\"\"\"\n",
        "Animal: {animal}\n",
        "\n",
        "Description: {description}\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "2gTtDV0QQA8f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Generate a brief description for the given animal:\",\n",
        "    suffix=\"Animal: {input}\",\n",
        "    input_variables=[\"input\"]\n",
        ")"
      ],
      "metadata": {
        "id": "it8ac7IkPZRc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_text(prompt.format(input=\"tiger\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "wxHIFncRRfAV",
        "outputId": "4c4f17e0-52e5-4cfe-903f-73564c678d2f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Generate a brief description for the given animal:\n\n\nAnimal: cat\n\nDescription: A furry feline with retractable claws and independent nature.\n\n\n\nAnimal: elephant\n\nDescription: A large mammal with a long trunk and tusks, known for its intelligence.\n\n\n\nAnimal: penguin\n\nDescription: A flightless bird adapted to swimming, with a tuxedo-like appearance.\n\n\nAnimal: tiger"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Language chain\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "display_text(chain.invoke({\"input\": \"tiger\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "xXzBL2J4RmFt",
        "outputId": "922d780b-5755-4241-bb91-138f412d00d9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Description: A powerful and majestic big cat with striking orange fur and black stripes, known for its strength and solitary behavior."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chain of Thought (COT)\n",
        "\n",
        "Chain-of-thought (CoT) prompting generates a sequence of reasoning steps to arrive at a final answer, which is particularly beneficial for complex reasoning tasks in large models. It enhances problem-solving by making the reasoning process explicit, thus improving accuracy in complicated scenarios. For simpler tasks, the benefits of CoT are less pronounced. [Source](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)"
      ],
      "metadata": {
        "id": "8gsfTzMXTGtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fWoVFuPQgHVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cot_template = \"\"\"Solve the following problem step by step:\n",
        "\n",
        "Problem: {problem}\n",
        "\n",
        "Let's approach this step-by-step, use as many steps as required:\n",
        "1)\n",
        "2)\n",
        "3)\n",
        "...\n",
        "\n",
        "Therefore, the final answer is:\"\"\""
      ],
      "metadata": {
        "id": "LITGwROESB2a"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=cot_template,\n",
        "    input_variables=[\"problem\"]\n",
        ")"
      ],
      "metadata": {
        "id": "WGfMdQKxTbO-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Chain\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "display_text(chain.invoke({\"problem\": \"What is the capital of France?\"}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "VNpPYqXKTedb",
        "outputId": "638ec3e6-0086-4102-e19f-1333085c20c1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Let's solve the problem step by step:\n\n1) Identify the country in question: France.\n2) Recall the common knowledge or facts about France: it is a country located in Western Europe.\n3) Consider the political and administrative aspects of France: every country has a capital city where the government is located.\n4) Reflect on historical and cultural knowledge: Paris is widely known as the most significant city in France, often associated with its culture, history, and importance as a capital.\n5) Verify the information: Confirm that Paris is recognized as the capital of France in various credible sources, such as geography books, encyclopedias, or authoritative websites.\n\nTherefore, the final answer is: Paris."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_text(chain.invoke({\"problem\": \"What is the speed of a car if it travels 400 miles per 3 hours.\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "FIhS53CVUByO",
        "outputId": "f993b290-51d2-4acc-82c0-6733b709ebf3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Let's solve the problem step by step:\n\n1) **Identify the given values**: We know the distance the car travels is 400 miles and the time taken is 3 hours.\n\n2) **Recall the formula for speed**: Speed is calculated using the formula:\n   \\[\n   \\text{Speed} = \\frac{\\text{Distance}}{\\text{Time}}\n   \\]\n\n3) **Substitute the values into the formula**: Now we will substitute the distance and time into the speed formula:\n   \\[\n   \\text{Speed} = \\frac{400 \\text{ miles}}{3 \\text{ hours}}\n   \\]\n\n4) **Perform the division**: Now we need to divide 400 by 3:\n   \\[\n   \\text{Speed} = 133.33 \\text{ miles per hour} \\quad (\\text{approximately})\n   \\]\n\n5) **Round the answer if necessary**: Depending on the context, you might round the answer. Here, we can keep it as 133.33 miles per hour, or we could state it as approximately 133 miles per hour if rounded to the nearest whole number.\n\nTherefore, the final answer is:\n**The speed of the car is approximately 133.33 miles per hour.**"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "400/3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1nw47JsUWxu",
        "outputId": "d1aaa4c1-500f-4d38-f1b0-76b84f1605d7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "133.33333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retrieval Augmented Generation"
      ],
      "metadata": {
        "id": "bsnQu3LoVQ_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval-Augmented Generation (RAG) is a hybrid technique in natural language processing that combines retrieval-based models with generation-based models to improve text generation quality. It works by first retrieving relevant information from a large corpus or knowledge base and then using a generative model, like GPT, to produce text that is more accurate and contextually relevant. This approach enhances the ability of AI to generate fact-based answers or content, making it especially useful in applications like question answering and specialized knowledge tasks, where accuracy and up-to-date information are critical."
      ],
      "metadata": {
        "id": "nkGCJRDrgh5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUC-Aw0OVI29",
        "outputId": "93640df3-52f0-4ee3-e0e5-0bfa9438e96e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load, chunk\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "g7QBZmrxWDOR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunking\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "eBxxwKPdd3G0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Indexing\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "LJQbmr-jeFN1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore.similarity_search(\"What is Zero Shot Prompting?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYskBts2eHn9",
        "outputId": "28ef07c4-f60f-4e77-8239-0956e589434f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content=\"[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.\\nBasic Prompting#\\nZero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.\\nZero-Shot#\\nZero-shot learning is to simply feed the task text to the model and ask for results.\\n(All the sentiment analysis examples are from SST-2)\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nFew-shot#\"),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content='Instruction Prompting#\\nThe purpose of presenting few-shot examples in the prompt is to explain our intent to the model; in other words, describe the task instruction to the model in the form of demonstrations. However, few-shot can be expensive in terms of token usage and restricts the input length due to limited context length. So, why not just give the instruction directly?\\nInstructed LM (e.g. InstructGPT, natural instruction) finetunes a pretrained model with high-quality tuples of (task instruction, input, ground truth output) to make LM better understand user intention and follow instruction. RLHF (Reinforcement Learning from Human Feedback) is a common method to do so. The benefit of instruction following style fine-tuning improves the model to be more aligned with human intention and greatly reduces the cost of communication.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content='Few-shot CoT. It is to prompt the model with a few demonstrations, each containing manually written (or model-generated) high-quality reasoning chains.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content='References#\\n[1] Zhao et al. “Calibrate Before Use: Improving Few-shot Performance of Language Models.” ICML 2021\\n[2] Liu et al. “What Makes Good In-Context Examples for GPT-3?” arXiv preprint arXiv:2101.06804 (2021).\\n[3] Lu et al. “Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity.” ACL 2022\\n[4] Ye et al. “In-Context Instruction Learning.” arXiv preprint arXiv:2302.14691 (2023).\\n[5] Su et al. “Selective annotation makes language models better few-shot learners.” arXiv preprint arXiv:2209.01975 (2022).\\n[6] Rubin et al. “Learning to retrieve prompts for in-context learning.” NAACL-HLT 2022\\n[7] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[8] Wang et al. “Self-Consistency Improves Chain of Thought Reasoning in Language Models.” ICLR 2023.\\n[9] Diao et al. “Active Prompting with Chain-of-Thought for Large Language Models.” arXiv preprint arXiv:2302.12246 (2023).')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve and generate using the relevant snippets of the blog.\n",
        "def get_response(query):\n",
        "  retriever = vectorstore.as_retriever()\n",
        "\n",
        "  # RAG prompt template\n",
        "  prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "  def format_docs(docs):\n",
        "      return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "  rag_chain = (\n",
        "      {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "      | prompt\n",
        "      | llm\n",
        "      | StrOutputParser()\n",
        "  )\n",
        "\n",
        "  response = rag_chain.invoke(query)\n",
        "  return response"
      ],
      "metadata": {
        "id": "pqHx4yxgeOnA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_text(get_response(\"What is Zero Shot Prompting?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "id": "cddrK1IIefEF",
        "outputId": "318b8f97-2f29-4baf-cc85-ece7c4573d19"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Zero Shot Prompting is a method where a model is given a task description without any examples and is asked to produce results based solely on that input. It relies on the model's ability to generalize from its training data to understand and perform the task. This approach is often used in benchmarking the performance of language models."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_text(get_response(\"What chain of thought prompting and why it is useful?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "id": "JRlrINxoen30",
        "outputId": "9e4af556-f00a-44c6-c923-a1efc42467a5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Chain-of-thought (CoT) prompting involves generating a series of short sentences that outline reasoning step-by-step, ultimately leading to a final answer. This method is particularly beneficial for complex reasoning tasks, especially when using large language models with over 50 billion parameters, as it clarifies the thought process. For simpler tasks, the advantages of CoT prompting are less significant."
          },
          "metadata": {}
        }
      ]
    }
  ]
}